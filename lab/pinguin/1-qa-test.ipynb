{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Syst√®me de Q&A Ultra-Rapide sur Transcription Vocale\n",
    "\n",
    "Ce notebook impl√©mente un syst√®me de questions-r√©ponses rapide bas√© sur :\n",
    "- **Sentence Transformers** : Embeddings multilingues fran√ßais\n",
    "- **FAISS** : Recherche vectorielle ultra-rapide\n",
    "- Temps de r√©ponse : ~50-200ms\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ 1. Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (5.2.0)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.1-cp310-abi3-macosx_14_0_arm64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (2.2.5)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers) (4.46.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (68.2.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\n",
      "Downloading faiss_cpu-1.13.1-cp310-abi3-macosx_14_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers faiss-cpu numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1218 09:00:01.907000 87432 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß 2. Classe FastQASystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastQASystem:\n",
    "    \"\"\"Syst√®me de Q&A ultra-rapide bas√© sur la recherche vectorielle\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "        \"\"\"\n",
    "        Initialise le syst√®me avec un mod√®le d'embeddings.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Nom du mod√®le SentenceTransformer\n",
    "                       Options: \n",
    "                       - 'paraphrase-multilingual-MiniLM-L12-v2' (rapide, bon)\n",
    "                       - 'paraphrase-multilingual-mpnet-base-v2' (plus lent, meilleur)\n",
    "        \"\"\"\n",
    "        print(f\"üì¶ Chargement du mod√®le {model_name}...\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.segments = []\n",
    "        print(\"‚úì Mod√®le charg√©!\")\n",
    "        \n",
    "    def prepare_transcription(self, transcription: str, window_size: int = 1) -> List[str]:\n",
    "        \"\"\"\n",
    "        D√©coupe la transcription en segments avec contexte.\n",
    "        \n",
    "        Args:\n",
    "            transcription: Texte de la transcription\n",
    "            window_size: Nombre de phrases avant/apr√®s √† inclure pour le contexte\n",
    "            \n",
    "        Returns:\n",
    "            Liste de segments avec contexte\n",
    "        \"\"\"\n",
    "        # D√©coupe par phrases\n",
    "        sentences = re.split(r'[.!?]+', transcription)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        segments = []\n",
    "        \n",
    "        # Cr√©e des segments avec fen√™tre de contexte\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            context = []\n",
    "            \n",
    "            # Ajoute les phrases pr√©c√©dentes\n",
    "            for j in range(max(0, i - window_size), i):\n",
    "                context.append(sentences[j])\n",
    "            \n",
    "            # Phrase actuelle\n",
    "            context.append(sentence)\n",
    "            \n",
    "            # Ajoute les phrases suivantes\n",
    "            for j in range(i + 1, min(len(sentences), i + window_size + 1)):\n",
    "                context.append(sentences[j])\n",
    "            \n",
    "            segments.append(' '.join(context))\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def index_transcription(self, transcription: str, window_size: int = 1):\n",
    "        \"\"\"\n",
    "        Indexe la transcription pour recherche rapide.\n",
    "        \n",
    "        Args:\n",
    "            transcription: Texte de la transcription\n",
    "            window_size: Taille de la fen√™tre de contexte\n",
    "        \"\"\"\n",
    "        print(\"\\nüîÑ Pr√©paration des segments...\")\n",
    "        self.segments = self.prepare_transcription(transcription, window_size)\n",
    "        print(f\"   ‚Üí {len(self.segments)} segments cr√©√©s\")\n",
    "        \n",
    "        print(\"üîÑ Encodage des segments...\")\n",
    "        start = time.time()\n",
    "        embeddings = self.model.encode(\n",
    "            self.segments, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        print(f\"   ‚Üí Encodage termin√© en {time.time() - start:.2f}s\")\n",
    "        \n",
    "        print(\"üîÑ Cr√©ation de l'index FAISS...\")\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # Inner Product pour similarit√© cosinus\n",
    "        \n",
    "        # Normalise pour utiliser la similarit√© cosinus\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        print(f\"‚úì Indexation termin√©e! ({self.index.ntotal} vecteurs)\")\n",
    "    \n",
    "    def search(self, question: str, top_k: int = 3) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Recherche les segments les plus pertinents.\n",
    "        \n",
    "        Args:\n",
    "            question: Question pos√©e\n",
    "            top_k: Nombre de r√©sultats √† retourner\n",
    "            \n",
    "        Returns:\n",
    "            Liste de tuples (segment, score de similarit√©)\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Le syst√®me n'a pas √©t√© index√©. Appelle d'abord index_transcription()\")\n",
    "        \n",
    "        # Encode la question\n",
    "        question_embedding = self.model.encode([question], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(question_embedding)\n",
    "        \n",
    "        # Recherche dans l'index\n",
    "        scores, indices = self.index.search(question_embedding, top_k)\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in zip(indices[0], scores[0]):\n",
    "            results.append((self.segments[idx], float(score)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def answer(self, question: str, min_confidence: float = 0.3) -> dict:\n",
    "        \"\"\"\n",
    "        R√©pond √† la question de mani√®re naturelle.\n",
    "        \n",
    "        Args:\n",
    "            question: Question pos√©e\n",
    "            min_confidence: Score minimum pour consid√©rer la r√©ponse valide\n",
    "            \n",
    "        Returns:\n",
    "            Dictionnaire avec 'answer', 'confidence', 'time_ms'\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        results = self.search(question, top_k=1)\n",
    "        \n",
    "        elapsed_ms = (time.time() - start) * 1000\n",
    "        \n",
    "        if not results:\n",
    "            return {\n",
    "                'answer': \"D√©sol√©, je n'ai pas trouv√© d'information sur ce sujet.\",\n",
    "                'confidence': 0.0,\n",
    "                'time_ms': elapsed_ms\n",
    "            }\n",
    "        \n",
    "        best_match, score = results[0]\n",
    "        \n",
    "        if score < min_confidence:\n",
    "            return {\n",
    "                'answer': \"Hmm, je ne suis pas s√ªr d'avoir compris ta question. Peux-tu reformuler ?\",\n",
    "                'confidence': score,\n",
    "                'time_ms': elapsed_ms\n",
    "            }\n",
    "        \n",
    "        answer = self._format_answer(best_match, score)\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'confidence': score,\n",
    "            'time_ms': elapsed_ms,\n",
    "            'raw_segment': best_match\n",
    "        }\n",
    "    \n",
    "    def _format_answer(self, text: str, confidence: float) -> str:\n",
    "        \"\"\"Ajoute un peu de naturel √† la r√©ponse\"\"\"\n",
    "        text = text.strip()\n",
    "        \n",
    "        if confidence > 0.8:\n",
    "            prefixes = [\"Voil√† : \", \"Ah oui ! \", \"Exactement : \", \"\"]\n",
    "        elif confidence > 0.5:\n",
    "            prefixes = [\"D'apr√®s ce que j'ai : \", \"Il semblerait que : \", \"\"]\n",
    "        else:\n",
    "            prefixes = [\"Je pense que : \", \"Peut-√™tre que : \"]\n",
    "        \n",
    "        prefix = random.choice(prefixes)\n",
    "        return f\"{prefix}{text}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù 3. Exemple de transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Transcription charg√©e!\n",
      "Longueur: 663 caract√®res\n"
     ]
    }
   ],
   "source": [
    "# Transcription d'exemple (remplace par ta vraie transcription)\n",
    "transcription_exemple = \"\"\"\n",
    "John est arriv√© ce matin avec son bob rose flashy, cette sacr√© canaille. \n",
    "Tout le monde a rigol√© en le voyant. \n",
    "Il a dit que c'√©tait un cadeau de sa grand-m√®re Josette.\n",
    "Marie portait une casquette bleue marine tr√®s √©l√©gante.\n",
    "Le bob de John est vraiment voyant, on ne peut pas le manquer.\n",
    "On l'a surnomm√© le flamant rose apr√®s √ßa.\n",
    "Sophie a apport√© des croissants pour tout le monde.\n",
    "Les croissants venaient de la boulangerie du coin de la rue.\n",
    "Pierre a racont√© une blague mais personne n'a ri.\n",
    "Il √©tait un peu vex√© mais il a fait semblant de rire aussi.\n",
    "La r√©union a commenc√© √† 9h pr√©cises.\n",
    "Le directeur a annonc√© de bonnes nouvelles sur les ventes du trimestre.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìÑ Transcription charg√©e!\")\n",
    "print(f\"Longueur: {len(transcription_exemple)} caract√®res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 4. Initialisation et indexation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Chargement du mod√®le paraphrase-multilingual-MiniLM-L12-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc1a33aa54344d6a09d28b7cb395429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcc964a023d4b428b13af49ded7e3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a7ef0672d04bff9f94ab9339fa6a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7304e403591d48b3936c2f2c22ee5e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3029d96fa440699b64e48d17a590ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440ffd23ad544387a75a43cde01f36a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f61b8c8781944718ca1c2a407807084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646112eec1f0406da607ce3db7820841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c2601bd0fd4cc183d6d6a3c44dd7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6bb65b45ab4fb584bb0f508fb696d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Mod√®le charg√©!\n"
     ]
    }
   ],
   "source": [
    "# Initialise le syst√®me\n",
    "qa_system = FastQASystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Pr√©paration des segments...\n",
      "   ‚Üí 12 segments cr√©√©s\n",
      "üîÑ Encodage des segments...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b14e0436644ac485cec2983aa8bfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Üí Encodage termin√© en 0.97s\n",
      "üîÑ Cr√©ation de l'index FAISS...\n",
      "‚úì Indexation termin√©e! (12 vecteurs)\n"
     ]
    }
   ],
   "source": [
    "# Indexe la transcription\n",
    "qa_system.index_transcription(transcription_exemple, window_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ 5. Test avec des questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste de questions de test\n",
    "questions_test = [\n",
    "    \"De quelle couleur est le bob de John ?\",\n",
    "    \"Qui porte une casquette ?\",\n",
    "    \"Quel surnom a √©t√© donn√© √† John ?\",\n",
    "    \"Qui a apport√© des croissants ?\",\n",
    "    \"D'o√π viennent les croissants ?\",\n",
    "    \"√Ä quelle heure a commenc√© la r√©union ?\",\n",
    "    \"Qu'a annonc√© le directeur ?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üß™ TEST DU SYST√àME DE Q&A\n",
      "======================================================================\n",
      "\n",
      "‚ùì Q: De quelle couleur est le bob de John ?\n",
      "üí¨ R: D'apr√®s ce que j'ai : John est arriv√© ce matin avec son bob rose flashy, cette sacr√© canaille Tout le monde a rigol√© en le voyant\n",
      "üìä Confiance: 60.32%\n",
      "‚ö° Temps: 695ms\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚ùì Q: Qui porte une casquette ?\n",
      "üí¨ R: Je pense que : Marie portait une casquette bleue marine tr√®s √©l√©gante Le bob de John est vraiment voyant, on ne peut pas le manquer On l'a surnomm√© le flamant rose apr√®s √ßa\n",
      "üìä Confiance: 43.25%\n",
      "‚ö° Temps: 337ms\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚ùì Q: Quel surnom a √©t√© donn√© √† John ?\n",
      "üí¨ R: Il semblerait que : John est arriv√© ce matin avec son bob rose flashy, cette sacr√© canaille Tout le monde a rigol√© en le voyant Il a dit que c'√©tait un cadeau de sa grand-m√®re Josette\n",
      "üìä Confiance: 56.27%\n",
      "‚ö° Temps: 14ms\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚ùì Q: Qui a apport√© des croissants ?\n",
      "üí¨ R: Il semblerait que : Sophie a apport√© des croissants pour tout le monde Les croissants venaient de la boulangerie du coin de la rue Pierre a racont√© une blague mais personne n'a ri\n",
      "üìä Confiance: 64.83%\n",
      "‚ö° Temps: 191ms\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚ùì Q: D'o√π viennent les croissants ?\n",
      "üí¨ R: On l'a surnomm√© le flamant rose apr√®s √ßa Sophie a apport√© des croissants pour tout le monde Les croissants venaient de la boulangerie du coin de la rue\n",
      "üìä Confiance: 61.30%\n",
      "‚ö° Temps: 12ms\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚ùì Q: √Ä quelle heure a commenc√© la r√©union ?\n",
      "üí¨ R: D'apr√®s ce que j'ai : La r√©union a commenc√© √† 9h pr√©cises Le directeur a annonc√© de bonnes nouvelles sur les ventes du trimestre\n",
      "üìä Confiance: 59.61%\n",
      "‚ö° Temps: 200ms\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚ùì Q: Qu'a annonc√© le directeur ?\n",
      "üí¨ R: Il semblerait que : Il √©tait un peu vex√© mais il a fait semblant de rire aussi La r√©union a commenc√© √† 9h pr√©cises Le directeur a annonc√© de bonnes nouvelles sur les ventes du trimestre\n",
      "üìä Confiance: 60.93%\n",
      "‚ö° Temps: 14ms\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test des questions\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ TEST DU SYST√àME DE Q&A\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for question in questions_test:\n",
    "    result = qa_system.answer(question)\n",
    "    \n",
    "    print(f\"‚ùì Q: {question}\")\n",
    "    print(f\"üí¨ R: {result['answer']}\")\n",
    "    print(f\"üìä Confiance: {result['confidence']:.2%}\")\n",
    "    print(f\"‚ö° Temps: {result['time_ms']:.0f}ms\")\n",
    "    print(\"-\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 6. Recherche avanc√©e (top 3 r√©sultats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Question: Parle-moi de John\n",
      "\n",
      "Top 3 r√©sultats:\n",
      "\n",
      "1. Score: 62.43%\n",
      "   John est arriv√© ce matin avec son bob rose flashy, cette sacr√© canaille Tout le monde a rigol√© en le voyant\n",
      "\n",
      "2. Score: 56.93%\n",
      "   John est arriv√© ce matin avec son bob rose flashy, cette sacr√© canaille Tout le monde a rigol√© en le voyant Il a dit que c'√©tait un cadeau de sa grand-m√®re Josette\n",
      "\n",
      "3. Score: 49.57%\n",
      "   Le bob de John est vraiment voyant, on ne peut pas le manquer On l'a surnomm√© le flamant rose apr√®s √ßa Sophie a apport√© des croissants pour tout le monde\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemple de recherche avec plusieurs r√©sultats\n",
    "question = \"Parle-moi de John\"\n",
    "results = qa_system.search(question, top_k=3)\n",
    "\n",
    "print(f\"üîç Question: {question}\\n\")\n",
    "print(\"Top 3 r√©sultats:\\n\")\n",
    "\n",
    "for i, (segment, score) in enumerate(results, 1):\n",
    "    print(f\"{i}. Score: {score:.2%}\")\n",
    "    print(f\"   {segment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 7. Test interactif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: De quelle couleur est le bob de John ?\n",
      "üí¨ R√©ponse: John est arriv√© ce matin avec son bob rose flashy, cette sacr√© canaille Tout le monde a rigol√© en le voyant\n",
      "\n",
      "üìä D√©tails:\n",
      "   - Confiance: 60.32%\n",
      "   - Temps: 53ms\n",
      "   - Segment source: John est arriv√© ce matin avec son bob rose flashy, cette sacr√© canaille Tout le monde a rigol√© en le voyant\n"
     ]
    }
   ],
   "source": [
    "# Pose ta propre question ici\n",
    "ma_question = \"De quelle couleur est le bob de John ?\"  # Modifie cette ligne\n",
    "\n",
    "result = qa_system.answer(ma_question)\n",
    "\n",
    "print(f\"‚ùì Question: {ma_question}\")\n",
    "print(f\"üí¨ R√©ponse: {result['answer']}\")\n",
    "print(f\"\\nüìä D√©tails:\")\n",
    "print(f\"   - Confiance: {result['confidence']:.2%}\")\n",
    "print(f\"   - Temps: {result['time_ms']:.0f}ms\")\n",
    "if 'raw_segment' in result:\n",
    "    print(f\"   - Segment source: {result['raw_segment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 8. Benchmark de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä STATISTIQUES DE PERFORMANCE\n",
      "========================================\n",
      "Temps moyen:  12.8ms\n",
      "Temps m√©dian: 12.8ms\n",
      "Temps min:    9.8ms\n",
      "Temps max:    60.1ms\n",
      "\n",
      "üéØ Total de 140 requ√™tes test√©es\n"
     ]
    }
   ],
   "source": [
    "# Test de performance sur plusieurs questions\n",
    "import statistics\n",
    "\n",
    "times = []\n",
    "for _ in range(20):\n",
    "    for q in questions_test:\n",
    "        result = qa_system.answer(q)\n",
    "        times.append(result['time_ms'])\n",
    "\n",
    "print(\"üìä STATISTIQUES DE PERFORMANCE\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Temps moyen:  {statistics.mean(times):.1f}ms\")\n",
    "print(f\"Temps m√©dian: {statistics.median(times):.1f}ms\")\n",
    "print(f\"Temps min:    {min(times):.1f}ms\")\n",
    "print(f\"Temps max:    {max(times):.1f}ms\")\n",
    "print(f\"\\nüéØ Total de {len(times)} requ√™tes test√©es\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

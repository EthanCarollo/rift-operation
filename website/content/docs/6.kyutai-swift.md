---
title: "Transcription Temps Réel avec Kyutai"
description: "Architecture client-serveur pour la transcription vocale temps réel utilisant Kyutai STT."
category: "Lab"
tags: ["KyutAI", "STT", "Real-time", "Voice"]
---

# Transcription Temps Réel avec Kyutai

Ce module fait partie du **module Pinguin** (dans l'écosystème Rift) et permet une transcription vocale en temps réel extrêmement efficace grâce au modèle **Kyutai STT**.

![Démonstration de la transcription en temps réel](/kyutai/kyutai-proto-test.gif)

![Prototype en conditions réelles](/kyutai/kyutai-proto-irl.gif)

## Pourquoi Kyutai ?

- **Rapidité** : Modèle optimisé pour le streaming audio, latence minimale
- **Spécialisé STT** : Conçu spécifiquement pour la transcription vocale (Speech-to-Text)
- **Support multilingue** : Français et Anglais natifs
- **Léger** : Fonctionne efficacement sur Apple Silicon via MLX

---

L'architecture est basée sur un modèle **client-serveur** avec communication WebSocket.

> [!NOTE]
> Bien que l'exécution locale sur mobile ait été envisagée, la taille du modèle (~1 milliard de paramètres) et les besoins en calcul rendent l'utilisation d'un serveur distant nécessaire pour maintenir une fluidité optimale. Le serveur WebSocket permet de déporter la charge de calcul tout en conservant une latence极为 faible.

```
┌─────────────────────┐    WebSocket     ┌─────────────────────┐
│   App iOS (Swift)   │ ───────────────► │   Serveur Python    │
│   Audio 24kHz       │                  │   Kyutai STT        │
│                     │ ◄─────────────── │                     │
│                     │   Texte transcrit │                     │
└─────────────────────┘                  └─────────────────────┘
```

---

## Serveur Python (FastAPI)

Le serveur charge le modèle Kyutai et expose un endpoint WebSocket pour recevoir l'audio et renvoyer la transcription.

### Chargement du modèle

```python
from huggingface_hub import hf_hub_download
from moshi_mlx import models, utils
import rustymimi
import sentencepiece

def load_model():
    hf_repo = "kyutai/stt-1b-en_fr-mlx"
    
    # Téléchargement automatique depuis HuggingFace
    config_path = hf_hub_download(hf_repo, "config.json", local_dir="kyutai-model")
    
    # Initialisation du modèle
    lm_config = models.LmConfig.from_config_dict(config_dict)
    model = models.Lm(lm_config)
    model.set_dtype(mx.bfloat16)
    
    # Tokenizers audio et texte
    audio_tokenizer = rustymimi.Tokenizer(mimi_weights, num_codebooks=mimi_codebooks)
    text_tokenizer = sentencepiece.SentencePieceProcessor(tokenizer_path)
```

### Endpoint WebSocket

```python
@app.websocket("/ws")
async def audio_websocket(websocket: WebSocket):
    await websocket.accept()
    
    # Générateur pour cette session
    local_gen = models.LmGen(
        model=model,
        max_steps=4096,
        text_sampler=utils.Sampler(top_k=25, temp=0),
        audio_sampler=utils.Sampler(top_k=250, temp=0.8),
    )
    
    while True:
        # Réception des chunks audio
        data = await websocket.receive_bytes()
        audio_chunk = np.frombuffer(data, dtype=np.float32)
        
        # Encodage audio → tokens
        audio_chunk = audio_chunk[None, None, :]
        other_audio_tokens = audio_tokenizer.encode_step(audio_chunk)
        
        # Génération texte
        text_token = local_gen.step(single_frame[0])
        
        if text_token not in (0, 3):
            text = text_tokenizer.id_to_piece(text_token[0].item())
            text = text.replace("▁", " ")
            await websocket.send_text(text)
```

### Lancer le serveur

```bash
cd iot/rift-pinguin/server
pip install -r requirements.txt
python main.py
```

Le serveur affiche son adresse locale pour la connexion client :
```
Local network address: http://192.168.x.x:8000
```

---

## Client iOS (Swift)

L'application iOS capture l'audio du microphone, le convertit en 24kHz mono, et l'envoie au serveur via WebSocket.

### Configuration audio

```swift
// Capture audio et conversion vers 24kHz
let targetFormat = AVAudioFormat(
    commonFormat: .pcmFormatFloat32, 
    sampleRate: 24000, 
    channels: 1, 
    interleaved: false
)

let converter = AVAudioConverter(from: hardwareFormat, to: targetFormat)

inputNode.installTap(onBus: 0, bufferSize: 4800, format: hardwareFormat) { buffer, time in
    // Conversion vers format cible
    converter.convert(to: outputBuffer, error: &error, withInputFrom: inputBlock)
    self.sendAudio(buffer: outputBuffer)
}
```

### Envoi WebSocket

```swift
private func sendAudio(buffer: AVAudioPCMBuffer) {
    let frameLength = Int(buffer.frameLength)
    let dataSize = frameLength * MemoryLayout<Float>.size
    let data = Data(bytes: floatChannelData[0], count: dataSize)
    
    let message = URLSessionWebSocketTask.Message.data(data)
    socket?.send(message) { error in
        if let error = error {
            print("Send error: \(error)")
        }
    }
}
```

### Réception de la transcription

```swift
private func receiveMessage() {
    socket?.receive { result in
        switch result {
        case .success(let message):
            if case .string(let text) = message {
                self.transcribedText += text  // Affichage temps réel
            }
            self.receiveMessage()  // Continue d'écouter
        case .failure(let error):
            print("Receive error: \(error)")
        }
    }
}
```

---

## Intégration dans Pinguin

Ce module de transcription sera intégré dans l'écosystème **Pinguin** pour permettre :

- **Commandes vocales** : Interaction mains-libres avec l'assistant
- **Dictée temps réel** : Transcription instantanée pour notes et messages
- **Accessibilité** : Support pour utilisateurs avec besoins spécifiques

> [!TIP]
> Le modèle fonctionne en streaming : chaque chunk audio (80ms) génère immédiatement des tokens de texte, permettant une transcription quasi-instantanée.

---

## Prérequis

### Serveur
- Python 3.10+
- Mac Apple Silicon (M1/M2/M3/M4) pour MLX
- ~2Go d'espace disque pour le modèle

### Client iOS
- iOS 17+ / macOS 14+
- Permission microphone (`NSMicrophoneUsageDescription`)
- Connexion réseau local avec le serveur

---

## Dépannage

| Problème | Solution |
|----------|----------|
| Erreur `0Hz` sur le client | Le microphone n'est pas accessible, vérifier les permissions |
| Pas de transcription | Vérifier que le serveur reçoit bien les chunks (logs serveur) |
| Latence élevée | Vérifier la connexion réseau, préférer le WiFi local |
| Modèle non trouvé | Le téléchargement HuggingFace se fait automatiquement au premier lancement |

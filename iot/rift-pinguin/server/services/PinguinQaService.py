import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from typing import List, Tuple, Dict, Any
import re
import time
import random

class PinguinQaService:
    """
    Syst√®me de Q&A ultra-rapide bas√© sur la recherche vectorielle (FAISS + Sentence Transformers).
    Transpos√© depuis le notebook lab/pinguin/1-qa-test.ipynb.
    """
    
    def __init__(self, model_name: str = 'paraphrase-multilingual-MiniLM-L12-v2', db_path: str = "transcription_db.txt"):
        """
        Initialise le service.
        """
        self.model_name = model_name
        self.db_path = db_path
        self.model = None
        self.index = None
        self.segments = []
        self.is_loaded = False
        
    def load_model(self):
        """
        Charge le mod√®le d'embeddings et restaure l'historique si pr√©sent.
        """
        if self.is_loaded:
            return
            
        print(f"üì¶ Chargement du mod√®le de Q&A: {self.model_name}...")
        self.model = SentenceTransformer(self.model_name)
        
        # Restauration de la base de donn√©es (fichier texte)
        import os
        if os.path.exists(self.db_path):
            print(f"üìÇ Restauration de la base de donn√©es depuis {self.db_path}...")
            with open(self.db_path, "r", encoding="utf-8") as f:
                history = f.read()
                if history.strip():
                    self.index_transcription(history, save_to_db=False)
        
        self.is_loaded = True
        print("‚úì Mod√®le Q&A charg√©!")
        
    def prepare_transcription(self, transcription: str, window_size: int = 1) -> List[str]:
        """
        D√©coupe la transcription en segments avec contexte.
        """
        # D√©coupe par phrases (points, points d'interrogation ou d'exclamation)
        sentences = re.split(r'[.!?]+', transcription)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        segments = []
        
        # Cr√©e des segments avec fen√™tre de contexte
        for i, sentence in enumerate(sentences):
            context = []
            
            # Ajoute les phrases pr√©c√©dentes
            for j in range(max(0, i - window_size), i):
                context.append(sentences[j])
            
            # Phrase actuelle
            context.append(sentence)
            
            # Ajoute les phrases suivantes
            for j in range(i + 1, min(len(sentences), i + window_size + 1)):
                context.append(sentences[j])
            
            segments.append(' '.join(context))
        
        return segments
    
    def index_transcription(self, transcription: str, window_size: int = 1, save_to_db: bool = True):
        """
        Indexe la transcription pour recherche rapide.
        """
        if not transcription.strip():
            print("‚ö†Ô∏è Transcription vide, rien √† indexer.")
            return

        print("üîÑ Indexation de la transcription...")
        
        # Sauvegarde dans la base de donn√©es si demand√©
        if save_to_db:
            with open(self.db_path, "a", encoding="utf-8") as f:
                f.write(transcription + "\n")
            
            # Re-charger tout pour l'indexation (si on veut que l'index contienne TOUT)
            with open(self.db_path, "r", encoding="utf-8") as f:
                full_history = f.read()
            self.segments = self.prepare_transcription(full_history, window_size)
        else:
            self.segments = self.prepare_transcription(transcription, window_size)
        
        if not self.segments:
            return

        # Encodage des segments
        embeddings = self.model.encode(
            self.segments, 
            show_progress_bar=False,
            convert_to_numpy=True
        )
        
        # Cr√©ation de l'index FAISS
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # Inner Product pour similarit√© cosinus (sur vecteurs normalis√©s)
        
        # Normalise pour utiliser la similarit√© cosinus
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings)
        
        print(f"‚úì Indexation termin√©e! ({self.index.ntotal} vecteurs)")
    
    def search(self, question: str, top_k: int = 3) -> List[Tuple[str, float]]:
        """
        Recherche les segments les plus pertinents.
        """
        if self.index is None:
            return []
        
        # Encode la question
        question_embedding = self.model.encode([question], convert_to_numpy=True)
        faiss.normalize_L2(question_embedding)
        
        # Recherche dans l'index
        scores, indices = self.index.search(question_embedding, top_k)
        
        results = []
        for idx, score in zip(indices[0], scores[0]):
            if idx != -1: # FAISS renvoie -1 si pas assez de r√©sultats
                results.append((self.segments[idx], float(score)))
        
        return results
    
    def answer(self, question: str, min_confidence: float = 0.3) -> Dict[str, Any]:
        """
        R√©pond √† la question de mani√®re naturelle en cherchant dans l'index.
        """
        start_time = time.time()
        
        results = self.search(question, top_k=1)
        
        elapsed_ms = (time.time() - start_time) * 1000
        
        if not results:
            return {
                'answer': "D√©sol√©, je n'ai pas encore assez de contenu √† analyser.",
                'confidence': 0.0,
                'time_ms': elapsed_ms
            }
        
        best_match, score = results[0]
        
        if score < min_confidence:
            return {
                'answer': "Hmm, je ne suis pas s√ªr d'avoir cette information. Peux-tu pr√©ciser ?",
                'confidence': score,
                'time_ms': elapsed_ms
            }
        
        answer = self._format_answer(best_match, score)
        
        return {
            'answer': answer,
            'confidence': score,
            'time_ms': elapsed_ms,
            'raw_segment': best_match
        }
    
    def _format_answer(self, text: str, confidence: float) -> str:
        """Ajoute un peu de naturel √† la r√©ponse"""
        text = text.strip()
        
        if confidence > 0.8:
            prefixes = ["Voil√† : ", "Ah oui ! ", "Exactement : ", ""]
        elif confidence > 0.5:
            prefixes = ["D'apr√®s ce que j'ai : ", "Il semblerait que : ", ""]
        else:
            prefixes = ["Je pense que : ", "Peut-√™tre que : "]
        
        prefix = random.choice(prefixes)
        return f"{prefix}{text}"
